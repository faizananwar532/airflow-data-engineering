# üöÄ Complete Airflow GitOps Setup - Step by Step Guide

This document provides a comprehensive overview of everything we accomplished in setting up Airflow with ArgoCD GitOps workflow.

## üìã **What We Built**

- ‚úÖ **ArgoCD** - GitOps controller for automatic deployments
- ‚úÖ **Airflow** - Data orchestration platform with MySQL backend
- ‚úÖ **MySQL Integration** - External MySQL database on EC2 (54.167.107.216:3306)
- ‚úÖ **S3 Logging** - Remote logging to S3 bucket (airflow-logs-ravi)
- ‚úÖ **GitHub Sync** - Automatic DAG synchronization from GitHub repository
- ‚úÖ **LoadBalancer** - External access via AWS ELB
- ‚úÖ **Secure GitOps** - No sensitive data in Git repository

## üéØ **Final Result**

### **Access Information:**
- **Airflow UI**: `http://a5efc61fb77e546bbab234e8b272c638-1361814829.us-east-1.elb.amazonaws.com:8080`
- **Username**: `admin`
- **Password**: `admin`
- **ArgoCD UI**: `http://a5558ceac349a49fea6a5b10b3cf30ca-3800ac6597e1a6b5.elb.us-east-1.amazonaws.com`

### **GitOps Workflow:**
1. Add/modify DAGs in `dags/` folder
2. Commit and push to GitHub
3. ArgoCD detects changes automatically (within 3 minutes)
4. Airflow pods restart with new DAGs
5. New DAGs appear in Airflow UI

---

## üìö **Complete Step-by-Step Process**

### **üéØ QUICK START - First Time Deployment (Run in Order)**

```bash
# 1. Navigate to project directory
cd airflow-data-engineering

# 2. Install ArgoCD
cd argocd
./install-argocd.sh

# 3. Get ArgoCD admin password
ARGOCD_PASSWORD=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d)
echo "ArgoCD Admin Password: $ARGOCD_PASSWORD"

# 4. Get ArgoCD URL
ARGOCD_URL=$(kubectl get svc argocd-server-loadbalancer -n argocd -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
echo "ArgoCD URL: http://$ARGOCD_URL"

# 5. Configure sensitive values (set your actual credentials)
export MYSQL_CONNECTION=""
export AWS_ACCESS_KEY_ID=""
export AWS_SECRET_ACCESS_KEY=""
export S3_LOG_BUCKET=""
export FERNET_KEY=""

# Optional: Customize MySQL (if different from above)
export MYSQL_HOST="54.167.107.216"
export MYSQL_USER="myuser"
export MYSQL_PASSWORD="mypassword"
export MYSQL_DATABASE="mydb"

./configure-sensitive-values.sh

# 6. Deploy Airflow via GitOps (creates MySQL secrets automatically)
./deploy-airflow-gitops.sh

# 7. Clean up sensitive files
rm airflow-application-with-secrets.yaml

# 8. Get Airflow URL
AIRFLOW_URL=$(kubectl get svc airflow-gitops-webserver -n airflow -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
echo "Airflow URL: http://$AIRFLOW_URL:8080"
echo "Airflow Login: admin/admin"

# 9. Monitor deployment
kubectl get pods -n airflow -w
```

**‚è±Ô∏è Total Time**: ~15-20 minutes  
**üë§ Default Airflow Login**: admin/admin  
**üîë ArgoCD Login**: admin/[password from step 3]

---

### **Phase 1: ArgoCD Installation**

#### Step 1: Install ArgoCD
```bash
cd airflow-data-engineering/argocd
./install-argocd.sh
```

**What it did:**
- Created `argocd` namespace
- Installed ArgoCD core components
- Created LoadBalancer service for external access
- Generated admin password (retrieve with command below)

#### Step 2: Get ArgoCD Admin Password
```bash
# Get ArgoCD admin password
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d && echo

# Get ArgoCD URL
kubectl get svc argocd-server-loadbalancer -n argocd
```

#### Step 3: Access ArgoCD UI
- **URL**: Use the LoadBalancer hostname from Step 2
- **Username**: `admin`
- **Password**: Use the password from Step 2

---

### **Phase 2: Secure Configuration Setup**

#### Step 3: Create Public Values File
Created `airflow-1.15/mysql-github-values-public.yaml` with:
- ‚úÖ All Airflow configuration (safe to commit)
- ‚úÖ Placeholder values for sensitive data
- ‚úÖ GitHub sync configuration
- ‚úÖ MySQL backend configuration
- ‚úÖ S3 logging configuration

#### Step 4: Update .gitignore
```gitignore
# Sensitive Airflow configuration files
mysql-values.yaml
mysql-github-values.yaml

# ArgoCD files with sensitive data
airflow-application-with-secrets.yaml
*-with-secrets.yaml

# Temporary files with credentials
*.tmp
*.temp
*-temp.yaml
```

#### Step 5: Commit Safe Files
```bash
git add .gitignore
git add airflow-1.15/mysql-github-values-public.yaml
git add argocd/
git commit -m "üîê Add secure ArgoCD GitOps setup"
git push origin main
```

---

### **üõ†Ô∏è ArgoCD Scripts Overview**

The `argocd/` directory contains several scripts. Here's the correct order to run them:

| Script | Purpose | When to Run |
|--------|---------|-------------|
| `install-argocd.sh` | Install ArgoCD in cluster | **First** - Run once per cluster |
| `configure-sensitive-values.sh` | Generate secure config with your credentials | **Second** - After setting environment variables |
| `deploy-airflow-gitops.sh` | Deploy Airflow via ArgoCD | **Third** - Main deployment script |
| `test-dag-sync.py` | Create test DAG for GitOps verification | **Optional** - After deployment |

### **üîê Automated MySQL Secrets Creation**

The `deploy-airflow-gitops.sh` script now **automatically creates MySQL secrets**. You can customize MySQL settings with environment variables:

```bash
# Optional: Customize MySQL settings (defaults shown)
export MYSQL_HOST="54.167.107.216"        # Default: 54.167.107.216
export MYSQL_PORT="3306"                  # Default: 3306  
export MYSQL_USER="myuser"                # Default: myuser
export MYSQL_PASSWORD="mypassword"        # Default: mypassword
export MYSQL_DATABASE="mydb"              # Default: mydb

# Run deployment (will create secrets automatically)
./deploy-airflow-gitops.sh
```

**What secrets are created automatically:**
- `airflow-mysql-metadata`: `mysql://user:pass@host:port/db`
- `airflow-mysql-result-backend`: `db+mysql://user:pass@host:port/db`

### **Phase 3: Configuration Files Setup**

---

### **Phase 4: Sensitive Values Configuration**

#### Step 8: Configure Sensitive Values
```bash
# Set environment variables
export MYSQL_CONNECTION=""
export AWS_ACCESS_KEY_ID=""
export AWS_SECRET_ACCESS_KEY=""
export S3_LOG_BUCKET=""
export FERNET_KEY=""

# Generate secure configuration
./configure-sensitive-values.sh
```

**What it created:**
- `airflow-application-with-secrets.yaml` - ArgoCD Application with real credentials
- **Environment variable mapping:**
  - `env[0]` ‚Üí `AIRFLOW_CONN_MYSQL`
  - `env[1]` ‚Üí `AIRFLOW_CONN_MYSQL_DEFAULT`
  - `env[3]` ‚Üí `AWS_ACCESS_KEY_ID`
  - `env[4]` ‚Üí `AWS_SECRET_ACCESS_KEY`
  - `env[8]` ‚Üí `AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER`
  - `env[10]` ‚Üí `AIRFLOW__CORE__FERNET_KEY`

---

### **Phase 5: ArgoCD Application Deployment**

#### Step 9: Deploy ArgoCD Application
```bash
kubectl apply -f airflow-application-with-secrets.yaml
```

**What it deployed:**
- ArgoCD Application pointing to GitHub repository
- Helm chart: `airflow-1.15`
- Values file: `mysql-github-values-public.yaml`
- Sensitive values injected via Helm parameters

#### Step 10: Monitor Deployment
```bash
# Check ArgoCD application
kubectl get application airflow-gitops -n argocd

# Check Airflow pods
kubectl get pods -n airflow

# Check services
kubectl get svc -n airflow
```

---

### **Phase 6: Troubleshooting & Fixes**

#### Issue 1: Configuration Parameter Mapping Error
**Problem**: Init containers were failing due to incorrect environment variable indices.

**Root Cause**: 
- We were setting `env[7]` (AIRFLOW__LOGGING__REMOTE_LOGGING) to S3 bucket path
- Should have been setting `env[8]` (AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER)

**Solution**:
1. Fixed parameter mapping in `configure-sensitive-values.sh`
2. Deleted and redeployed ArgoCD application
3. Recreated all secrets and ConfigMaps

#### Issue 2: Stuck Deletion
**Problem**: ArgoCD application deletion was stuck due to finalizers.

**Solution**:
```bash
# Remove finalizer to force deletion
kubectl patch application airflow-gitops -n argocd -p '{"metadata":{"finalizers":null}}' --type=merge

# Clean up all resources
kubectl delete all --all -n airflow
kubectl delete configmaps,secrets,pvc --all -n airflow
```

---

### **Phase 7: Final Deployment Success**

#### Step 11: Fresh Deployment
After fixing the configuration issues:
```bash
# Deploy corrected ArgoCD application
kubectl apply -f airflow-application-with-secrets.yaml

# Recreate secrets
kubectl create secret generic airflow-mysql-metadata --namespace airflow --from-literal=connection="mysql://myuser:mypassword@54.167.107.216:3306/mydb"
kubectl create secret generic airflow-mysql-result-backend --namespace airflow --from-literal=connection="db+mysql://myuser:mypassword@54.167.107.216:3306/mydb"

# Recreate ConfigMap
kubectl apply -f ../airflow-1.15/airflow-advanced-test-dag-configmap.yaml -n airflow
```

#### Step 12: Verification
**Final Status:**
```
NAME                                        READY   STATUS    RESTARTS   AGE
airflow-gitops-redis-0                      1/1     Running   0          2m17s
airflow-gitops-scheduler-664d9d5bcf-twzg7   2/3     Running   0          2m17s
airflow-gitops-statsd-7b4df4db7c-dl74s      1/1     Running   0          2m17s
airflow-gitops-triggerer-0                  2/2     Running   0          2m17s
airflow-gitops-webserver-78cbc996f4-w6ftc   2/2     Running   0          2m17s
airflow-gitops-worker-0                     3/3     Running   0          2m17s
```

**Git Sync Verification:**
```bash
kubectl logs airflow-gitops-scheduler-664d9d5bcf-twzg7 -n airflow -c dags-git-sync --tail=5
```
Shows DAGs syncing from GitHub:
- `dag_s3_sync_test.py`
- `dag_s3_to_mysql_etl.py`
- `dag_test_mysql.py`
- `dag_test_mysql_version.py`

---

## üîß **Technical Architecture**

### **Components:**
1. **ArgoCD**: GitOps controller monitoring GitHub repository
2. **Airflow Scheduler**: Orchestrates DAG execution (with Git sync sidecar)
3. **Airflow Webserver**: Web UI and API (with Git sync sidecar)
4. **Airflow Workers**: Execute tasks (with Git sync sidecar)
5. **Redis**: Message broker for Celery
6. **MySQL**: External database for metadata storage
7. **S3**: Remote logging storage

### **Data Flow:**
```
GitHub Repository ‚Üí ArgoCD ‚Üí Kubernetes ‚Üí Airflow Pods ‚Üí Git Sync ‚Üí DAGs Folder
                                      ‚Üì
MySQL Database ‚Üê Airflow Metadata ‚Üê Airflow Components
                                      ‚Üì
S3 Bucket ‚Üê Task Logs ‚Üê Airflow Tasks
```

### **Security Model:**
- ‚úÖ **Git Repository**: Only public configuration, no secrets
- ‚úÖ **Kubernetes Secrets**: MySQL connection strings
- ‚úÖ **ArgoCD Parameters**: Sensitive values injected at deployment
- ‚úÖ **Local Files**: Temporary files with credentials (gitignored)

---

## üéØ **Key Files Created/Modified**

### **Safe to Commit (in Git):**
- `airflow-1.15/mysql-github-values-public.yaml` - Public Helm values
- `argocd/airflow-application.yaml` - ArgoCD Application template
- `argocd/configure-sensitive-values.sh` - Credential configuration script
- `argocd/install-argocd.sh` - ArgoCD installation script
- `argocd/deploy-airflow-gitops.sh` - GitOps deployment script
- `.gitignore` - Updated with sensitive file patterns

### **Never Commit (Protected by .gitignore):**
- `airflow-application-with-secrets.yaml` - Contains real credentials
- `mysql-github-values.yaml` - Original file with hardcoded secrets

---

## üöÄ **Usage Instructions**

### **Daily Workflow:**
1. **Add new DAG**: Create `.py` file in `dags/` folder
2. **Test locally**: Validate DAG syntax
3. **Commit**: `git add`, `git commit`, `git push`
4. **Wait**: ArgoCD detects change (within 3 minutes)
5. **Verify**: Check Airflow UI for new DAG

### **Monitoring Commands:**
```bash
# Check ArgoCD sync status
kubectl get application airflow-gitops -n argocd

# Watch Airflow pods
kubectl get pods -n airflow -w

# Check Git sync logs
kubectl logs -n airflow -l component=scheduler -c dags-git-sync --tail=10

# Force ArgoCD sync
kubectl patch application airflow-gitops -n argocd -p '{"operation":{"sync":{}}}' --type merge
```

### **Access URLs:**
- **Airflow**: `http://a5efc61fb77e546bbab234e8b272c638-1361814829.us-east-1.elb.amazonaws.com:8080`
- **ArgoCD**: `http://a5558ceac349a49fea6a5b10b3cf30ca-3800ac6597e1a6b5.elb.us-east-1.amazonaws.com`

---

## üõ†Ô∏è **Maintenance & Operations**

### **Update Credentials:**
```bash
# Set new values
export MYSQL_CONNECTION="new-connection-string"
export AWS_ACCESS_KEY_ID="new-access-key"
# ... other variables

# Regenerate configuration
./configure-sensitive-values.sh

# Apply update
kubectl apply -f airflow-application-with-secrets.yaml

# Clean up
rm airflow-application-with-secrets.yaml
```

### **Scale Components:**
Edit the ArgoCD Application parameters:
```yaml
- name: scheduler.replicas
  value: "2"
- name: workers.replicas
  value: "3"
```

### **Backup & Recovery:**
- **MySQL Database**: Regular backups of external MySQL
- **S3 Logs**: Retained in S3 bucket
- **DAGs**: Stored in Git repository
- **Configuration**: All infrastructure as code

---

## üéâ **Success Metrics**

‚úÖ **Deployment**: All pods running successfully
‚úÖ **GitOps**: Automatic sync from GitHub working
‚úÖ **Database**: MySQL integration successful
‚úÖ **Logging**: S3 remote logging configured
‚úÖ **Security**: No sensitive data in Git
‚úÖ **Access**: External LoadBalancer working
‚úÖ **Monitoring**: ArgoCD UI showing healthy status

**Total Setup Time**: ~2 hours (including troubleshooting)
**Components Deployed**: 15+ Kubernetes resources
**DAGs Synced**: 4 DAGs from GitHub repository
**Security Level**: Production-ready with secret management

---

## üìö **Next Steps & Enhancements**

### **Immediate:**
- [ ] Change default admin password
- [ ] Set up RBAC for additional users
- [ ] Configure SSL certificates for HTTPS

### **Future Enhancements:**
- [ ] Add Prometheus monitoring
- [ ] Set up alerting with AlertManager
- [ ] Implement multi-environment (dev/staging/prod)
- [ ] Add automated testing pipeline
- [ ] Configure backup automation

---

## üÜò **Troubleshooting - Common First-Time Issues**

### **Issue 1: ArgoCD Password Not Found**
```bash
# Error: secret "argocd-initial-admin-secret" not found
# Solution: Wait for ArgoCD to fully start
kubectl get pods -n argocd
kubectl wait --for=condition=Ready pod -l app.kubernetes.io/name=argocd-server -n argocd --timeout=300s

# Then retry getting password
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d
```

### **Issue 2: LoadBalancer Pending External IP**
```bash
# Check LoadBalancer status
kubectl get svc -n argocd
kubectl get svc -n airflow

# If stuck in "Pending", check AWS Load Balancer Controller
kubectl get pods -n kube-system | grep aws-load-balancer-controller

# Alternative: Use port-forward
kubectl port-forward svc/argocd-server-loadbalancer 8080:80 -n argocd
kubectl port-forward svc/airflow-gitops-webserver 8081:8080 -n airflow
```

### **Issue 3: Airflow Pods CrashLoopBackOff**
```bash
# Check pod logs
kubectl logs -n airflow -l component=scheduler --tail=20

# Common cause: Environment variable mapping error
# Solution: Verify environment variables in configure-sensitive-values.sh
# Ensure env[8] = S3 bucket, env[10] = Fernet key (not env[7] and env[9])

# Force redeploy
kubectl delete application airflow-gitops -n argocd
./deploy-airflow-gitops.sh
```

### **Issue 4: MySQL Connection Failed**
```bash
# Check MySQL secrets
kubectl get secrets -n airflow | grep mysql
kubectl get secret airflow-mysql-metadata -n airflow -o yaml

# Test MySQL connection from cluster
kubectl run mysql-test --rm -i --tty --image=mysql:8.0 -- mysql -h54.167.107.216 -umyuser -pmypassword mydb

# Update MySQL credentials
export MYSQL_USER="correct-user"
export MYSQL_PASSWORD="correct-password"
./deploy-airflow-gitops.sh
```

### **Issue 5: DAGs Not Syncing from GitHub**
```bash
# Check Git sync logs
kubectl logs -n airflow -l component=scheduler -c dags-git-sync --tail=10

# Verify repository access
git clone https://github.com/faizananwar532/airflow-data-engineering.git

# Force ArgoCD sync
kubectl patch application airflow-gitops -n argocd -p '{"operation":{"sync":{}}}' --type merge
```

### **Issue 6: Script Permission Denied**
```bash
# Make scripts executable
chmod +x argocd/*.sh

# Or run with bash
bash argocd/install-argocd.sh
bash argocd/deploy-airflow-gitops.sh
```

### **üö® Emergency Reset**
If everything is broken, start fresh:
```bash
# Delete everything
kubectl delete namespace airflow --ignore-not-found=true
kubectl delete namespace argocd --ignore-not-found=true

# Wait for deletion
kubectl get namespaces

# Start over
./install-argocd.sh
```

---

**üéä Congratulations! Your GitOps Airflow deployment is complete and operational! üéä**
